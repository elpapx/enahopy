#!/usr/bin/env python3
"""
Script de prueba para lectura de archivos locales ENAHO
Prueba la funcionalidad de lectura sin necesidad de descargar
"""

import sys
import tempfile
import pandas as pd
import numpy as np
from pathlib import Path

# Agregar el directorio del proyecto al path
project_dir = Path(__file__).parent.parent  # Ajustar seg√∫n ubicaci√≥n
sys.path.insert(0, str(project_dir))


def create_test_stata_file(filepath):
    """Crear un archivo Stata de prueba simulando estructura ENAHO"""
    # Crear DataFrame con estructura t√≠pica de ENAHO
    np.random.seed(42)
    n_rows = 100

    # Crear arrays primero y luego convertir
    conglome = np.random.randint(100000, 999999, n_rows)
    vivienda = np.random.randint(1, 20, n_rows)
    hogar = np.random.randint(1, 3, n_rows)
    codperso = np.random.randint(1, 8, n_rows)
    ubigeo = np.random.randint(10101, 250101, n_rows)

    df = pd.DataFrame({
        'conglome': conglome.astype(str),
        'vivienda': pd.Series(vivienda).astype(str).str.zfill(2),
        'hogar': hogar.astype(str),
        'codperso': pd.Series(codperso).astype(str).str.zfill(2),
        'ubigeo': ubigeo.astype(str),
        'dominio': np.random.randint(1, 8, n_rows),
        'estrato': np.random.randint(1, 5, n_rows),
        'p203': np.random.choice([1, 2], n_rows),  # Parentesco
        'p207': np.random.choice([1, 2], n_rows),  # Sexo
        'p208a': np.random.randint(0, 99, n_rows),  # Edad
        'factor07': np.random.uniform(50, 500, n_rows),  # Factor de expansi√≥n
        'mes': np.random.randint(1, 13, n_rows),
        'a√±o': 2023
    })

    # Guardar como Stata
    df.to_stata(filepath, write_index=False)
    return df


def test_local_reader_basic():
    """Test b√°sico del lector local"""
    from loader.io.local_reader import ENAHOLocalReader
    from loader.utils import get_file_info

    print("\n" + "=" * 60)
    print("TEST 1: LECTURA B√ÅSICA DE ARCHIVO LOCAL")
    print("=" * 60)

    with tempfile.TemporaryDirectory() as tmpdir:
        # Crear archivo de prueba
        test_file = Path(tmpdir) / "test_enaho.dta"
        original_df = create_test_stata_file(test_file)
        print(f"‚úÖ Archivo de prueba creado: {test_file.name}")
        print(f"   - Filas: {len(original_df)}")
        print(f"   - Columnas: {len(original_df.columns)}")

        try:
            # Test 1: Obtener informaci√≥n del archivo
            print("\nüìä Informaci√≥n del archivo:")
            file_info = get_file_info(str(test_file), verbose=False)

            print(f"   - Formato: {file_info['file_info'].get('file_format', 'N/A')}")
            print(f"   - Total columnas: {file_info['total_columns']}")
            print(f"   - Tama√±o: {file_info['file_info'].get('file_size_mb', 0):.2f} MB")
            # Nota: 'enaho_columns_found' puede no existir en todas las implementaciones
            if 'enaho_columns_found' in file_info:
                print(f"   - Columnas ENAHO detectadas: {file_info['enaho_columns_found']}")

            # Test 2: Leer archivo usando el reader
            print("\nüìñ Leyendo archivo con ENAHOLocalReader:")
            reader = ENAHOLocalReader(
                file_path=str(test_file),
                verbose=False
            )

            # Obtener informaci√≥n b√°sica
            info = reader.get_file_info()
            print(f"‚úÖ Reader creado, formato: {info.get('file_format', 'N/A')}")
            print(f"   - Columnas disponibles: {info.get('total_columns', 0)}")

            return True

        except Exception as e:
            print(f"‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
            return False


def test_column_selection():
    """Test de selecci√≥n espec√≠fica de columnas"""
    from loader.utils import read_enaho_file

    print("\n" + "=" * 60)
    print("TEST 2: SELECCI√ìN DE COLUMNAS")
    print("=" * 60)

    with tempfile.TemporaryDirectory() as tmpdir:
        test_file = Path(tmpdir) / "test_enaho.dta"
        create_test_stata_file(test_file)

        try:
            # Test 1: Leer solo columnas clave
            print("\nüîë Leyendo solo columnas clave:")
            columns_to_read = ["conglome", "vivienda", "hogar", "codperso"]

            data, validation = read_enaho_file(
                file_path=str(test_file),
                columns=columns_to_read,
                ignore_missing_columns=False,
                verbose=False
            )

            print(f"‚úÖ Columnas solicitadas: {columns_to_read}")
            print(f"‚úÖ Columnas le√≠das: {list(data.columns)}")
            assert list(data.columns) == columns_to_read

            # Test 2: Intentar leer columnas que no existen
            print("\nüîç Probando columnas inexistentes:")
            data, validation = read_enaho_file(
                file_path=str(test_file),
                columns=["conglome", "columna_inexistente"],
                ignore_missing_columns=True,
                verbose=False
            )

            print(f"‚úÖ Columnas encontradas: {list(data.columns)}")
            print(f"‚ö†Ô∏è  Columnas faltantes ignoradas: {validation.missing_columns}")

            # Test 3: Case insensitive
            print("\nüî§ Probando case insensitive:")
            data, validation = read_enaho_file(
                file_path=str(test_file),
                columns=["CONGLOME", "Vivienda", "HoGaR"],
                case_sensitive=False,
                verbose=False
            )

            print(f"‚úÖ B√∫squeda case-insensitive funcion√≥")
            print(f"   Columnas encontradas: {list(data.columns)}")

            return True

        except Exception as e:
            print(f"‚ùå Error: {e}")
            return False


def test_chunked_reading():
    """Test de lectura por chunks para archivos grandes"""
    print("\n" + "=" * 60)
    print("TEST 3: LECTURA POR CHUNKS")
    print("=" * 60)

    with tempfile.TemporaryDirectory() as tmpdir:
        # Crear archivo m√°s grande
        test_file = Path(tmpdir) / "test_large.dta"
        np.random.seed(42)
        n_rows = 1000

        # Crear arrays primero
        conglome = np.random.randint(100000, 999999, n_rows)
        vivienda = np.random.randint(1, 20, n_rows)
        hogar = np.random.randint(1, 3, n_rows)

        large_df = pd.DataFrame({
            'conglome': conglome.astype(str),
            'vivienda': pd.Series(vivienda).astype(str).str.zfill(2),
            'hogar': hogar.astype(str),
            'factor07': np.random.uniform(50, 500, n_rows),
            'p208a': np.random.randint(0, 99, n_rows)
        })
        large_df.to_stata(test_file, write_index=False)

        try:
            print(f"üìÑ Archivo creado con {n_rows} filas")

            # Intentar leer con chunks si est√° disponible
            print("\nüì¶ Probando lectura de archivo grande:")

            # Opci√≥n 1: Usar pandas directamente para simular chunks
            try:
                # Leer en chunks usando pandas
                chunk_size = 250
                chunks_processed = 0
                total_rows = 0

                print(f"‚è≥ Leyendo en bloques de {chunk_size} filas...")

                # Leer por chunks
                full_data = pd.read_stata(test_file)

                # Simular procesamiento por chunks
                for i in range(0, len(full_data), chunk_size):
                    chunk = full_data.iloc[i:i + chunk_size]
                    chunks_processed += 1
                    total_rows += len(chunk)
                    print(f"   Chunk {chunks_processed}: {len(chunk)} filas")

                    if chunks_processed >= 4:  # Procesar solo primeros 4 chunks
                        break

                print(f"\n‚úÖ Chunks procesados: {chunks_processed}")
                print(f"‚úÖ Total filas le√≠das: {total_rows}")

            except Exception as e:
                print(f"‚ÑπÔ∏è  Lectura por chunks no disponible, leyendo completo")
                full_data = pd.read_stata(test_file)
                print(f"‚úÖ Archivo le√≠do completo: {len(full_data)} filas")

            return True

        except Exception as e:
            print(f"‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
            return False


def test_find_local_files():
    """Test de b√∫squeda de archivos locales"""
    from loader.utils import find_enaho_files

    print("\n" + "=" * 60)
    print("TEST 4: B√öSQUEDA DE ARCHIVOS LOCALES")
    print("=" * 60)

    with tempfile.TemporaryDirectory() as tmpdir:
        tmppath = Path(tmpdir)

        # Crear estructura de directorios y archivos
        (tmppath / "2023").mkdir()
        (tmppath / "2023" / "modulo_01").mkdir()
        (tmppath / "2023" / "modulo_34").mkdir()
        (tmppath / "2022").mkdir()

        # Crear archivos de prueba
        test_files = [
            tmppath / "2023" / "modulo_01" / "enaho01-2023-100.dta",
            tmppath / "2023" / "modulo_01" / "enaho01-2023-200.dta",
            tmppath / "2023" / "modulo_34" / "sumaria-2023.dta",
            tmppath / "2022" / "enaho01-2022-100.dta",
            tmppath / "otros.txt",  # Archivo no .dta
            tmppath / "test.csv"  # Otro formato
        ]

        for file in test_files:
            if file.suffix == '.dta':
                # Crear archivo Stata m√≠nimo
                pd.DataFrame({'test': [1, 2, 3]}).to_stata(file, write_index=False)
            else:
                file.touch()

        try:
            # Test 1: Buscar todos los .dta recursivamente
            print("\nüîç Buscando archivos .dta recursivamente:")
            found_files = find_enaho_files(str(tmppath), "*.dta", recursive=True)

            print(f"‚úÖ Archivos .dta encontrados: {len(found_files)}")
            for file in sorted(found_files):
                relative_path = Path(file).relative_to(tmppath)
                print(f"   - {relative_path}")

            assert len(found_files) == 4, f"Esperados 4 archivos .dta, encontrados {len(found_files)}"

            # Test 2: Buscar con patr√≥n espec√≠fico
            print("\nüîç Buscando archivos con patr√≥n 'enaho01*.dta':")
            pattern_files = find_enaho_files(str(tmppath), "enaho01*.dta", recursive=True)

            print(f"‚úÖ Archivos con patr√≥n encontrados: {len(pattern_files)}")
            for file in sorted(pattern_files):
                print(f"   - {Path(file).name}")

            # Test 3: Buscar solo en directorio espec√≠fico (no recursivo)
            print("\nüîç Buscando solo en directorio ra√≠z (no recursivo):")
            root_files = find_enaho_files(str(tmppath), "*", recursive=False)

            print(f"‚úÖ Archivos en ra√≠z: {len(root_files)}")
            assert len(root_files) == 2, f"Esperados 2 archivos en ra√≠z, encontrados {len(root_files)}"

            return True

        except Exception as e:
            print(f"‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
            return False


def test_validation_results():
    """Test del sistema de validaci√≥n de columnas"""
    from loader.io.local_reader import ENAHOLocalReader

    print("\n" + "=" * 60)
    print("TEST 5: SISTEMA DE VALIDACI√ìN")
    print("=" * 60)

    with tempfile.TemporaryDirectory() as tmpdir:
        test_file = Path(tmpdir) / "test_validation.dta"

        # Crear archivo con algunas columnas ENAHO y otras no
        df = pd.DataFrame({
            'conglome': ['123456'] * 10,
            'vivienda': ['01'] * 10,
            'hogar': ['1'] * 10,
            'columna_extra': range(10),
            'otro_campo': ['A'] * 10,
            'factor07': [100.5] * 10
        })
        df.to_stata(test_file, write_index=False)

        try:
            print("üìÑ Archivo de prueba creado")

            # Crear lector
            reader = ENAHOLocalReader(
                file_path=str(test_file),
                verbose=False
            )

            # Obtener informaci√≥n del archivo
            file_info = reader.get_file_info()
            print(f"\nüìä Informaci√≥n del archivo:")
            print(f"   - Total columnas: {file_info.get('total_columns', 0)}")
            print(f"   - Formato: {file_info.get('file_format', 'N/A')}")

            # Validar columnas espec√≠ficas
            validation = reader.validate_columns(['conglome', 'vivienda', 'hogar', 'codperso'])

            print("\nüîç Resultados de validaci√≥n:")
            print(f"   - Columnas totales en archivo: {validation.total_columns}")
            print(f"   - Columnas solicitadas encontradas: {validation.requested_columns_found}")
            print(f"   - Columnas faltantes: {validation.missing_columns}")

            if hasattr(validation, 'extra_columns'):
                print(f"   - Columnas extra: {validation.extra_columns}")

            # Obtener resumen si est√° disponible
            if hasattr(validation, 'get_summary'):
                print("\nüìù Resumen de validaci√≥n:")
                summary = validation.get_summary()
                print(summary)

            return True

        except Exception as e:
            print(f"‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
            return False


def test_memory_optimization():
    """Test de optimizaci√≥n de memoria"""
    from loader.utils import read_enaho_file

    print("\n" + "=" * 60)
    print("TEST 6: OPTIMIZACI√ìN DE MEMORIA")
    print("=" * 60)

    with tempfile.TemporaryDirectory() as tmpdir:
        test_file = Path(tmpdir) / "test_memory.dta"

        # Crear archivo con diferentes tipos de datos
        n_rows = 500
        df = pd.DataFrame({
            'id_text': ['ID' + str(i).zfill(6) for i in range(n_rows)],
            'categoria': np.random.choice(['A', 'B', 'C', 'D'], n_rows),
            'valor_int': np.random.randint(0, 100, n_rows),
            'valor_float': np.random.uniform(0, 1000, n_rows),
            'factor': np.random.uniform(50, 500, n_rows)
        })
        df.to_stata(test_file, write_index=False)

        try:
            print("\nüìä Leyendo archivo de prueba:")

            # Leer el archivo normalmente
            data, validation = read_enaho_file(
                file_path=str(test_file),
                verbose=False
            )

            if data is not None and len(data) > 0:
                memory_usage = data.memory_usage(deep=True).sum() / 1024 ** 2
                print(f"   - Filas: {len(data)}")
                print(f"   - Columnas: {len(data.columns)}")
                print(f"   - Memoria usada: {memory_usage:.2f} MB")

                # Mostrar tipos de datos
                print("\nüîç Tipos de datos:")
                for col in data.columns:
                    print(f"   - {col}: {data[col].dtype}")

                # Intentar optimizaci√≥n manual de tipos
                print("\n‚ö° Aplicando optimizaci√≥n manual de tipos:")

                # Convertir categor√≠as a tipo category si es posible
                for col in data.columns:
                    if data[col].dtype == 'object':
                        unique_ratio = len(data[col].unique()) / len(data[col])
                        if unique_ratio < 0.5:  # Si menos del 50% son √∫nicos
                            data[col] = data[col].astype('category')
                            print(f"   - {col} convertido a category")

                # Optimizar enteros
                for col in data.columns:
                    if 'int' in str(data[col].dtype):
                        max_val = data[col].max()
                        min_val = data[col].min()
                        if min_val >= 0 and max_val <= 255:
                            data[col] = data[col].astype('uint8')
                            print(f"   - {col} optimizado a uint8")
                        elif min_val >= -128 and max_val <= 127:
                            data[col] = data[col].astype('int8')
                            print(f"   - {col} optimizado a int8")

                memory_optimized = data.memory_usage(deep=True).sum() / 1024 ** 2
                reduction = (1 - memory_optimized / memory_usage) * 100

                print(f"\n‚úÖ Memoria despu√©s de optimizaci√≥n: {memory_optimized:.2f} MB")
                print(f"‚úÖ Reducci√≥n: {reduction:.1f}%")
            else:
                print("‚ö†Ô∏è  No se pudieron leer datos para la prueba de memoria")

            return True

        except Exception as e:
            print(f"‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
            return False
            return True

        except Exception as e:
            print(f"‚ùå Error: {e}")
            return False


def main():
    """Ejecutar todas las pruebas de lectura local"""
    print("\n" + "üöÄ PRUEBAS DE LECTURA LOCAL ".center(60, "="))
    print("=" * 60)

    tests = [
        ("Lectura B√°sica", test_local_reader_basic),
        ("Selecci√≥n de Columnas", test_column_selection),
        ("Lectura por Chunks", test_chunked_reading),
        ("B√∫squeda de Archivos", test_find_local_files),
        ("Sistema de Validaci√≥n", test_validation_results),
        ("Optimizaci√≥n de Memoria", test_memory_optimization)
    ]

    results = []
    for name, test_func in tests:
        print(f"\nüîÑ Ejecutando: {name}")
        try:
            success = test_func()
            results.append((name, success))
        except Exception as e:
            print(f"\n‚ùå Error no manejado en {name}: {e}")
            results.append((name, False))

    # Resumen
    print("\n" + "=" * 60)
    print("RESUMEN DE PRUEBAS DE LECTURA LOCAL")
    print("=" * 60)

    passed = 0
    for name, success in results:
        status = "‚úÖ PAS√ì" if success else "‚ùå FALL√ì"
        print(f"{status} - {name}")
        if success:
            passed += 1

    total = len(results)
    print(f"\nüìä Resultado: {passed}/{total} pruebas pasaron")

    if passed == total:
        print("üéâ ¬°Todas las pruebas de lectura local pasaron!")
    else:
        print(f"‚ö†Ô∏è  {total - passed} pruebas fallaron")

    return passed == total


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)