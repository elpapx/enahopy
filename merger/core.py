"""
ENAHO Merger - Clases Principales
================================

Implementaci√≥n de las clases principales ENAHOGeoMerger con
funcionalidades completas de fusi√≥n geogr√°fica y merge de m√≥dulos.
"""

import logging
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any

import pandas as pd
import numpy as np

# Importaciones internas
from .config import (
    GeoMergeConfiguration, ModuleMergeConfig, GeoValidationResult,
    ModuleMergeResult, TipoManejoErrores, TipoManejoDuplicados
)
from .exceptions import GeoMergeError, ModuleMergeError
from .geographic.validators import UbigeoValidator, TerritorialValidator, GeoDataQualityValidator
from .geographic.patterns import GeoPatternDetector
from .geographic.strategies import DuplicateStrategyFactory
from .modules.merger import ENAHOModuleMerger
from .modules.validator import ModuleValidator

# Importaciones opcionales del loader principal
try:
    from ..loader import ENAHOConfig, setup_logging, log_performance, CacheManager
except ImportError:
    # Fallback para uso independiente
    from dataclasses import dataclass


    @dataclass(frozen=True)
    class ENAHOConfig:
        cache_dir: str = ".enaho_cache"


    def setup_logging(verbose: bool = True, structured: bool = False, log_file: Optional[str] = None):
        logger = logging.getLogger('enaho_geo_merger')
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('[%(asctime)s] [%(levelname)s] %(name)s: %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO if verbose else logging.WARNING)
        return logger


    def log_performance(func):
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)

        return wrapper


    CacheManager = None


class ENAHOGeoMerger:
    """
    Fusionador geogr√°fico avanzado para datos INEI integrado con enaho-analyzer.
    EXTENDIDO con capacidades de merge entre m√≥dulos ENAHO.

    Proporciona funcionalidades completas para fusionar datos con informaci√≥n
    geogr√°fica, validaci√≥n de UBIGEO, detecci√≥n autom√°tica de patrones,
    estrategias flexibles de manejo de duplicados, y merge entre m√≥dulos ENAHO.
    """

    def __init__(self,
                 config: Optional[ENAHOConfig] = None,
                 geo_config: Optional[GeoMergeConfiguration] = None,
                 module_config: Optional[ModuleMergeConfig] = None,
                 verbose: bool = True,
                 structured_logging: bool = False,
                 log_file: Optional[str] = None):
        """
        Inicializa el merger geogr√°fico extendido.

        Args:
            config: Configuraci√≥n ENAHO general
            geo_config: Configuraci√≥n espec√≠fica de fusi√≥n geogr√°fica
            module_config: Configuraci√≥n espec√≠fica de merge entre m√≥dulos
            verbose: Si mostrar logs detallados
            structured_logging: Si usar logging estructurado
            log_file: Archivo para logs
        """
        self.config = config or ENAHOConfig()
        self.geo_config = geo_config or GeoMergeConfiguration()
        self.module_config = module_config or ModuleMergeConfig()
        self.logger = setup_logging(verbose, structured_logging, log_file)

        # Inicializar validadores geogr√°ficos
        self.ubigeo_validator = UbigeoValidator(self.logger)
        self.territorial_validator = TerritorialValidator(self.logger)
        self.pattern_detector = GeoPatternDetector(self.logger)
        self.quality_validator = GeoDataQualityValidator(self.logger)

        # Inicializar merger de m√≥dulos
        self.module_merger = ENAHOModuleMerger(self.module_config, self.logger)

        # Cache para datos geogr√°ficos
        self._geo_cache = {}
        if CacheManager and hasattr(self.config, 'cache_dir'):
            try:
                self.cache_manager = CacheManager(self.config.cache_dir)
            except Exception:
                self.cache_manager = None
        else:
            self.cache_manager = None

        self.logger.info("üó∫Ô∏è  ENAHOGeoMerger inicializado (Versi√≥n Refactorizada)")
        self.logger.info(f"   Nivel territorial objetivo: {self.geo_config.nivel_territorial_objetivo.value}")
        self.logger.info(f"   Validaci√≥n UBIGEO: {self.geo_config.tipo_validacion_ubigeo.value}")
        self.logger.info(f"   Merge de m√≥dulos habilitado: ‚úÖ")

    def _optimize_dataframe_memory(self, df: pd.DataFrame, name: str = "") -> pd.DataFrame:
        """Optimiza el uso de memoria del DataFrame"""
        if not self.geo_config.optimizar_memoria:
            return df

        initial_memory = df.memory_usage(deep=True).sum() / 1024 / 1024

        # Optimizar categ√≥ricas para columnas geogr√°ficas
        geo_columns = ['departamento', 'provincia', 'distrito', 'centro_poblado']
        for col in df.columns:
            if any(geo_term in col.lower() for geo_term in geo_columns):
                if df[col].dtype == 'object' and df[col].nunique() / len(df) < 0.5:
                    df[col] = df[col].astype('category')

        final_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
        saved = initial_memory - final_memory

        if saved > 0 and name:
            self.logger.info(f"üîß {name} memoria optimizada: {saved:.1f}MB ahorrados")

        return df

    @log_performance
    def validate_geographic_data(self, df: pd.DataFrame,
                                 columna_ubigeo: Optional[str] = None) -> GeoValidationResult:
        """
        Valida integralmente un DataFrame con datos geogr√°ficos.

        Args:
            df: DataFrame a validar
            columna_ubigeo: Columna con c√≥digos UBIGEO (None para autodetectar)

        Returns:
            Resultado completo de validaci√≥n
        """
        self.logger.info("üîç Iniciando validaci√≥n geogr√°fica completa")

        errors = []
        warnings = []
        quality_metrics = {}

        # Autodetectar columna UBIGEO si no se especifica
        if columna_ubigeo is None:
            columnas_geo = self.pattern_detector.detectar_columnas_geograficas(df)
            if 'ubigeo' in columnas_geo:
                columna_ubigeo = columnas_geo['ubigeo']
            else:
                errors.append("No se encontr√≥ columna UBIGEO")
                return GeoValidationResult(
                    is_valid=False, total_records=len(df), valid_ubigeos=0,
                    invalid_ubigeos=len(df), duplicate_ubigeos=0,
                    missing_coordinates=0, territorial_inconsistencies=0,
                    coverage_percentage=0.0, errors=errors, warnings=warnings,
                    quality_metrics=quality_metrics
                )

        total_records = len(df)

        # Validar UBIGEOs
        mask_validos, ubigeo_errors = self.ubigeo_validator.validar_serie_ubigeos(
            df[columna_ubigeo], self.geo_config.tipo_validacion_ubigeo
        )

        valid_ubigeos = mask_validos.sum()
        invalid_ubigeos = total_records - valid_ubigeos
        errors.extend(ubigeo_errors[:10])  # M√°ximo 10 errores de UBIGEO

        # Detectar duplicados
        duplicates_mask = df[columna_ubigeo].duplicated(keep=False)
        duplicate_ubigeos = duplicates_mask.sum()

        if duplicate_ubigeos > 0:
            warnings.append(f"Se encontraron {duplicate_ubigeos} UBIGEOs duplicados")

        # Validar coordenadas si est√°n disponibles
        missing_coordinates = 0
        if self.geo_config.validar_coordenadas:
            columnas_geo = self.pattern_detector.detectar_columnas_geograficas(df)
            if 'coordenada_x' in columnas_geo and 'coordenada_y' in columnas_geo:
                coord_x_col = columnas_geo['coordenada_x']
                coord_y_col = columnas_geo['coordenada_y']
                missing_coordinates = df[[coord_x_col, coord_y_col]].isnull().any(axis=1).sum()

        # Validar consistencia territorial
        territorial_inconsistencies = 0
        if self.geo_config.validar_consistencia_territorial:
            columnas_geo = self.pattern_detector.detectar_columnas_geograficas(df)
            inconsistencias = self.territorial_validator.validar_jerarquia_territorial(df, columnas_geo)
            territorial_inconsistencies = len(inconsistencias)
            warnings.extend(inconsistencias)

        # Calcular m√©tricas de calidad
        coverage_percentage = (valid_ubigeos / total_records * 100) if total_records > 0 else 0

        quality_metrics = {
            'completeness': coverage_percentage,
            'uniqueness': ((total_records - duplicate_ubigeos) / total_records * 100) if total_records > 0 else 0,
            'validity': (valid_ubigeos / total_records * 100) if total_records > 0 else 0,
            'consistency': (1 - territorial_inconsistencies / total_records) * 100 if total_records > 0 else 100
        }

        # Determinar si es v√°lido
        is_valid = (
                coverage_percentage >= 90 and  # Al menos 90% de cobertura
                quality_metrics['uniqueness'] >= 95 and  # Al menos 95% √∫nicos
                territorial_inconsistencies == 0  # Sin inconsistencias
        )

        result = GeoValidationResult(
            is_valid=is_valid,
            total_records=total_records,
            valid_ubigeos=valid_ubigeos,
            invalid_ubigeos=invalid_ubigeos,
            duplicate_ubigeos=duplicate_ubigeos,
            missing_coordinates=missing_coordinates,
            territorial_inconsistencies=territorial_inconsistencies,
            coverage_percentage=coverage_percentage,
            errors=errors,
            warnings=warnings,
            quality_metrics=quality_metrics
        )

        self.logger.info(f"‚úÖ Validaci√≥n completada - Cobertura: {coverage_percentage:.1f}%")
        return result

    def _handle_duplicates(self, df: pd.DataFrame, columna_union: str) -> pd.DataFrame:
        """Maneja duplicados usando la estrategia configurada"""
        duplicates_mask = df[columna_union].duplicated(keep=False)

        if not duplicates_mask.any():
            return df

        if self.geo_config.manejo_duplicados == TipoManejoDuplicados.ERROR:
            n_duplicates = duplicates_mask.sum()
            ubigeos_duplicados = df[duplicates_mask][columna_union].unique()
            raise GeoMergeError(
                f"Se encontraron {n_duplicates} duplicados en '{columna_union}'. "
                f"UBIGEOs afectados: {ubigeos_duplicados[:5]}"
            )

        strategy = DuplicateStrategyFactory.create_strategy(
            self.geo_config.manejo_duplicados, self.logger
        )
        return strategy.handle_duplicates(df, columna_union, self.geo_config)

    @log_performance
    def merge_geographic_data(self,
                              df_principal: pd.DataFrame,
                              df_geografia: pd.DataFrame,
                              columnas_geograficas: Optional[Dict[str, str]] = None,
                              columna_union: Optional[str] = None,
                              validate_before_merge: bool = None) -> Tuple[pd.DataFrame, GeoValidationResult]:
        """
        Fusiona datos principales con informaci√≥n geogr√°fica.

        Args:
            df_principal: DataFrame con datos principales
            df_geografia: DataFrame con informaci√≥n geogr√°fica
            columnas_geograficas: Mapeo espec√≠fico de columnas o None para autodetecci√≥n
            columna_union: Columna para el merge o None para usar configuraci√≥n
            validate_before_merge: Si validar antes de fusionar

        Returns:
            Tupla con (DataFrame fusionado, Resultado de validaci√≥n)
        """
        self.logger.info("üó∫Ô∏è  Iniciando fusi√≥n geogr√°fica avanzada")

        # Usar configuraci√≥n por defecto si no se especifica
        columna_union = columna_union or self.geo_config.columna_union
        validate_before_merge = validate_before_merge if validate_before_merge is not None else self.geo_config.generar_reporte_calidad

        # Validar que existe la columna de uni√≥n
        for df, nombre in zip([df_principal, df_geografia], ['principal', 'geograf√≠a']):
            if columna_union not in df.columns:
                raise GeoMergeError(f"Columna '{columna_union}' no encontrada en DataFrame {nombre}")

        # Validaci√≥n previa si est√° habilitada
        validation_result = None
        if validate_before_merge:
            validation_result = self.validate_geographic_data(df_geografia, columna_union)

            if not validation_result.is_valid and self.geo_config.manejo_errores == TipoManejoErrores.RAISE:
                raise GeoMergeError(f"Validaci√≥n geogr√°fica fall√≥:\n{validation_result.get_summary_report()}")

        # Autodetectar columnas geogr√°ficas si no se especifican
        if columnas_geograficas is None:
            columnas_geograficas = self.pattern_detector.detectar_columnas_geograficas(df_geografia)
            if not columnas_geograficas:
                raise GeoMergeError("No se pudieron detectar columnas geogr√°ficas autom√°ticamente")
            self.logger.info(f"üìç Columnas geogr√°ficas detectadas: {list(columnas_geograficas.keys())}")

        # Validar que las columnas especificadas existan
        columnas_faltantes = [col for col in columnas_geograficas.values()
                              if col not in df_geografia.columns]
        if columnas_faltantes:
            raise GeoMergeError(f"Columnas no encontradas en df_geografia: {columnas_faltantes}")

        # Optimizar memoria si est√° habilitado
        if self.geo_config.optimizar_memoria:
            df_geografia = self._optimize_dataframe_memory(df_geografia, "geograf√≠a")

        # Manejar duplicados
        df_geografia_limpio = self._handle_duplicates(df_geografia, columna_union)

        # Preparar DataFrame para merge
        columnas_seleccion = [columna_union] + list(columnas_geograficas.values())
        df_geo_para_merge = df_geografia_limpio[columnas_seleccion].copy()

        # Aplicar prefijos/sufijos a columnas
        mapeo_renombre = {}
        for tipo_geo, nombre_columna in columnas_geograficas.items():
            nuevo_nombre = f"{self.geo_config.prefijo_columnas}{tipo_geo}{self.geo_config.sufijo_columnas}"
            if nuevo_nombre != nombre_columna:  # Solo renombrar si es diferente
                mapeo_renombre[nombre_columna] = nuevo_nombre

        if mapeo_renombre:
            df_geo_para_merge = df_geo_para_merge.rename(columns=mapeo_renombre)

        # Realizar merge
        try:
            df_resultado = pd.merge(
                left=df_principal,
                right=df_geo_para_merge,
                on=columna_union,
                how='left',
                validate='m:1'
            )
        except pd.errors.MergeError as e:
            raise GeoMergeError(f"Error en la fusi√≥n de datos: {str(e)}")

        # Manejar valores faltantes
        columnas_geo_nuevas = list(mapeo_renombre.values()) if mapeo_renombre else list(columnas_geograficas.values())
        self._handle_missing_values(df_resultado, columnas_geo_nuevas)

        # Optimizar memoria del resultado
        if self.geo_config.optimizar_memoria:
            df_resultado = self._optimize_dataframe_memory(df_resultado, "resultado")

        # Reportar estad√≠sticas
        if self.geo_config.mostrar_estadisticas:
            self._report_merge_statistics(df_principal, df_resultado, columnas_geo_nuevas)

        self.logger.info(f"‚úÖ Fusi√≥n geogr√°fica completada: {df_resultado.shape}")

        return df_resultado, validation_result

    def _handle_missing_values(self, df: pd.DataFrame, columnas_geograficas: List[str]) -> None:
        """Maneja valores faltantes seg√∫n la configuraci√≥n"""
        if self.geo_config.manejo_errores == TipoManejoErrores.COERCE:
            for col in columnas_geograficas:
                if col in df.columns:
                    df[col] = df[col].fillna(self.geo_config.valor_faltante)

        elif self.geo_config.manejo_errores == TipoManejoErrores.RAISE:
            faltantes = df[columnas_geograficas].isnull().any(axis=1)
            if faltantes.any():
                n_faltantes = faltantes.sum()
                raise GeoMergeError(f"Se encontraron {n_faltantes} registros con informaci√≥n geogr√°fica faltante")

        elif self.geo_config.manejo_errores == TipoManejoErrores.LOG_WARNING:
            for col in columnas_geograficas:
                if col in df.columns:
                    n_missing = df[col].isnull().sum()
                    if n_missing > 0:
                        self.logger.warning(f"‚ö†Ô∏è  {n_missing} valores faltantes en columna '{col}'")

    def _report_merge_statistics(self, df_original: pd.DataFrame,
                                 df_resultado: pd.DataFrame, columnas_geo_nuevas: List[str]) -> None:
        """Reporta estad√≠sticas detalladas del merge"""
        n_original = len(df_original)
        n_resultado = len(df_resultado)

        if columnas_geo_nuevas and columnas_geo_nuevas[0] in df_resultado.columns:
            matches_exitosos = df_resultado[columnas_geo_nuevas[0]].notna().sum()
            tasa_match = (matches_exitosos / n_original) * 100

            self.logger.info(f"""
üìä Estad√≠sticas de fusi√≥n geogr√°fica:
   ‚Ä¢ Registros originales: {n_original:,}
   ‚Ä¢ Registros resultado: {n_resultado:,}
   ‚Ä¢ Matches exitosos: {matches_exitosos:,} ({tasa_match:.1f}%)
   ‚Ä¢ Registros sin match: {n_original - matches_exitosos:,}
   ‚Ä¢ Columnas geogr√°ficas a√±adidas: {len(columnas_geo_nuevas)}
            """)

    def extract_territorial_components(self, df: pd.DataFrame,
                                       columna_ubigeo: str) -> pd.DataFrame:
        """
        Extrae componentes territoriales jer√°rquicos de c√≥digos UBIGEO.

        Args:
            df: DataFrame con c√≥digos UBIGEO
            columna_ubigeo: Nombre de la columna con UBIGEO

        Returns:
            DataFrame con componentes territoriales extra√≠dos
        """
        self.logger.info(f"üóÇÔ∏è  Extrayendo componentes territoriales de '{columna_ubigeo}'")

        if columna_ubigeo not in df.columns:
            raise GeoMergeError(f"Columna '{columna_ubigeo}' no encontrada")

        componentes = self.ubigeo_validator.extraer_componentes_ubigeo(df[columna_ubigeo])

        # Combinar con DataFrame original
        result = pd.concat([df, componentes.drop('ubigeo', axis=1)], axis=1)

        self.logger.info(f"‚úÖ Componentes extra√≠dos: {list(componentes.columns)}")
        return result

    # =====================================================
    # FUNCIONALIDADES DE MERGE ENTRE M√ìDULOS
    # =====================================================

    def merge_modules(self,
                      left_df: pd.DataFrame,
                      right_df: pd.DataFrame,
                      left_module: str,
                      right_module: str,
                      merge_config: Optional[ModuleMergeConfig] = None) -> ModuleMergeResult:
        """
        Merge entre dos m√≥dulos ENAHO con validaciones espec√≠ficas.

        Args:
            left_df: DataFrame del m√≥dulo izquierdo
            right_df: DataFrame del m√≥dulo derecho
            left_module: C√≥digo del m√≥dulo izquierdo (ej: "01", "05")
            right_module: C√≥digo del m√≥dulo derecho
            merge_config: Configuraci√≥n espec√≠fica para este merge

        Returns:
            ModuleMergeResult con DataFrame combinado y m√©tricas
        """
        return self.module_merger.merge_modules(
            left_df, right_df, left_module, right_module, merge_config
        )

    def merge_multiple_modules(self,
                               modules_dict: Dict[str, pd.DataFrame],
                               base_module: str = "34",
                               merge_config: Optional[ModuleMergeConfig] = None) -> ModuleMergeResult:
        """
        Merge m√∫ltiples m√≥dulos secuencialmente.

        Args:
            modules_dict: Diccionario {codigo_modulo: dataframe}
            base_module: C√≥digo del m√≥dulo base para iniciar
            merge_config: Configuraci√≥n de merge

        Returns:
            ModuleMergeResult con todos los m√≥dulos combinados
        """
        return self.module_merger.merge_multiple_modules(
            modules_dict, base_module, merge_config
        )

    def merge_modules_with_geography(self,
                                     modules_dict: Dict[str, pd.DataFrame],
                                     df_geografia: pd.DataFrame,
                                     base_module: str = "34",
                                     merge_config: Optional[ModuleMergeConfig] = None,
                                     geo_config: Optional[GeoMergeConfiguration] = None) -> Tuple[
        pd.DataFrame, Dict[str, Any]]:
        """
        Combina m√∫ltiples m√≥dulos y luego agrega informaci√≥n geogr√°fica.

        Args:
            modules_dict: Diccionario con m√≥dulos ENAHO
            df_geografia: DataFrame con informaci√≥n geogr√°fica
            base_module: M√≥dulo base para iniciar merge
            merge_config: Configuraci√≥n para merge de m√≥dulos
            geo_config: Configuraci√≥n para merge geogr√°fico

        Returns:
            Tupla con DataFrame final y reporte completo
        """
        self.logger.info("üîóüó∫Ô∏è  Iniciando merge combinado: m√≥dulos + geograf√≠a")

        # 1. Merge entre m√≥dulos
        module_result = self.merge_multiple_modules(modules_dict, base_module, merge_config)

        self.logger.info(f"üìä M√≥dulos combinados: {len(module_result.merged_df)} registros")

        # 2. Merge con informaci√≥n geogr√°fica
        # Usar configuraci√≥n espec√≠fica si se proporciona
        original_geo_config = self.geo_config
        if geo_config:
            self.geo_config = geo_config

        try:
            geo_result, geo_validation = self.merge_geographic_data(
                df_principal=module_result.merged_df,
                df_geografia=df_geografia
            )
        finally:
            # Restaurar configuraci√≥n original
            self.geo_config = original_geo_config

        # 3. Combinar reportes
        combined_report = {
            'module_merge': module_result.merge_report,
            'geographic_merge': {
                'validation': geo_validation.to_dict() if geo_validation else None,
                'final_records': len(geo_result)
            },
            'overall_quality': self._assess_combined_quality(geo_result, module_result),
            'processing_summary': {
                'modules_processed': len(modules_dict),
                'base_module': base_module,
                'final_shape': geo_result.shape,
                'merge_sequence': module_result.merge_report.get('modules_sequence', ''),
                'geographic_coverage': geo_validation.coverage_percentage if geo_validation else 0
            }
        }

        self.logger.info(f"‚úÖ Merge combinado completado: {geo_result.shape}")
        return geo_result, combined_report

    def validate_module_compatibility(self,
                                      modules_dict: Dict[str, pd.DataFrame],
                                      merge_level: str = "hogar") -> Dict[str, Any]:
        """
        Valida compatibilidad entre m√∫ltiples m√≥dulos.

        Args:
            modules_dict: Diccionario con m√≥dulos a validar
            merge_level: Nivel de merge ("hogar", "persona", "vivienda")

        Returns:
            Reporte de compatibilidad detallado
        """
        from .config import ModuleMergeLevel

        level_enum = ModuleMergeLevel(merge_level)
        compatibility_report = {
            'overall_compatible': True,
            'merge_level': merge_level,
            'modules_analyzed': list(modules_dict.keys()),
            'pairwise_compatibility': {},
            'recommendations': [],
            'warnings': [],
            'potential_issues': []
        }

        # Validar cada par de m√≥dulos
        module_codes = list(modules_dict.keys())

        for i, module1 in enumerate(module_codes):
            for module2 in module_codes[i + 1:]:
                pair_key = f"{module1}-{module2}"

                compatibility = self.module_merger.validator.check_module_compatibility(
                    modules_dict[module1], modules_dict[module2],
                    module1, module2, level_enum
                )

                compatibility_report['pairwise_compatibility'][pair_key] = compatibility

                if not compatibility['compatible']:
                    compatibility_report['overall_compatible'] = False
                    compatibility_report['potential_issues'].append(
                        f"M√≥dulos {module1} y {module2}: {compatibility.get('error', 'Incompatible')}"
                    )

        # Generar recomendaciones
        if compatibility_report['overall_compatible']:
            compatibility_report['recommendations'].append(
                "‚úÖ Todos los m√≥dulos son compatibles para merge"
            )
        else:
            compatibility_report['recommendations'].append(
                "‚ö†Ô∏è  Algunos m√≥dulos presentan incompatibilidades. Revisar configuraci√≥n de merge."
            )

            # Sugerir nivel alternativo si es necesario
            if merge_level == "persona":
                compatibility_report['recommendations'].append(
                    "üí° Considere usar nivel 'hogar' para mayor compatibilidad"
                )

        return compatibility_report

    def _assess_combined_quality(self, final_df: pd.DataFrame,
                                 module_result: ModuleMergeResult) -> Dict[str, float]:
        """Eval√∫a la calidad combinada del resultado final"""

        # M√©tricas de completitud
        completeness = (1 - final_df.isnull().sum().sum() / (final_df.shape[0] * final_df.shape[1])) * 100

        # M√©tricas de consistencia (basado en llaves primarias)
        key_cols = [col for col in ['conglome', 'vivienda', 'hogar'] if col in final_df.columns]
        if key_cols:
            duplicates = final_df.duplicated(subset=key_cols).sum()
            consistency = (1 - duplicates / len(final_df)) * 100 if len(final_df) > 0 else 100
        else:
            consistency = 100

        # Score de m√≥dulos
        module_quality = module_result.quality_score

        # Score combinado
        weights = {'completeness': 0.4, 'consistency': 0.3, 'module_quality': 0.3}
        combined_score = (
                completeness * weights['completeness'] +
                consistency * weights['consistency'] +
                module_quality * weights['module_quality']
        )

        return {
            'completeness': completeness,
            'consistency': consistency,
            'module_quality': module_quality,
            'combined_score': combined_score,
            'quality_grade': self._get_quality_grade(combined_score)
        }

    def _get_quality_grade(self, score: float) -> str:
        """Asigna grado de calidad basado en score"""
        if score >= 90:
            return "A+ (Excelente)"
        elif score >= 80:
            return "A (Muy Bueno)"
        elif score >= 70:
            return "B (Bueno)"
        elif score >= 60:
            return "C (Aceptable)"
        elif score >= 50:
            return "D (Regular)"
        else:
            return "F (Deficiente)"

    # =====================================================
    # M√âTODOS DE AN√ÅLISIS Y UTILIDADES
    # =====================================================

    def analyze_merge_feasibility(self, df1: pd.DataFrame, df2: pd.DataFrame,
                                  merge_type: str = "geographic") -> Dict[str, Any]:
        """
        Analiza la viabilidad de merge entre dos DataFrames

        Args:
            df1, df2: DataFrames a analizar
            merge_type: Tipo de merge ("geographic" o "module")

        Returns:
            An√°lisis de viabilidad
        """
        analysis = {
            'feasible': True,
            'merge_type': merge_type,
            'size_compatibility': {},
            'structure_compatibility': {},
            'recommendations': [],
            'potential_issues': []
        }

        # An√°lisis de tama√±os
        size1 = len(df1)
        size2 = len(df2)

        analysis['size_compatibility'] = {
            'df1_size': size1,
            'df2_size': size2,
            'size_ratio': min(size1, size2) / max(size1, size2) if max(size1, size2) > 0 else 0,
            'memory_estimate_mb': (size1 + size2) * len(set(df1.columns) | set(df2.columns)) * 8 / 1024 / 1024
        }

        if merge_type == "geographic":
            # An√°lisis espec√≠fico para merge geogr√°fico
            geo_cols1 = self.pattern_detector.detectar_columnas_geograficas(df1)
            geo_cols2 = self.pattern_detector.detectar_columnas_geograficas(df2)

            common_geo = set(geo_cols1.keys()) & set(geo_cols2.keys())

            analysis['structure_compatibility'] = {
                'geo_columns_df1': len(geo_cols1),
                'geo_columns_df2': len(geo_cols2),
                'common_geo_levels': list(common_geo),
                'compatibility_score': len(common_geo) / max(len(geo_cols1), len(geo_cols2), 1)
            }

            if not common_geo:
                analysis['feasible'] = False
                analysis['potential_issues'].append("No hay niveles geogr√°ficos comunes")

        elif merge_type == "module":
            # An√°lisis espec√≠fico para merge de m√≥dulos
            key_cols = ['conglome', 'vivienda', 'hogar']
            missing1 = [col for col in key_cols if col not in df1.columns]
            missing2 = [col for col in key_cols if col not in df2.columns]

            analysis['structure_compatibility'] = {
                'missing_keys_df1': missing1,
                'missing_keys_df2': missing2,
                'key_compatibility': len(missing1) == 0 and len(missing2) == 0
            }

            if missing1 or missing2:
                analysis['feasible'] = False
                analysis['potential_issues'].append(f"Llaves faltantes: df1={missing1}, df2={missing2}")

        # Recomendaciones generales
        if analysis['size_compatibility']['memory_estimate_mb'] > 1000:  # 1GB
            analysis['recommendations'].append("Considere procesamiento por chunks para manejar memoria")

        if analysis['size_compatibility']['size_ratio'] < 0.1:
            analysis['recommendations'].append("Gran diferencia de tama√±os - verifique que sea correcto")

        return analysis

    def create_comprehensive_report(self, df: pd.DataFrame,
                                    include_geographic: bool = True,
                                    include_quality: bool = True) -> str:
        """
        Crea reporte integral de un DataFrame

        Args:
            df: DataFrame a analizar
            include_geographic: Si incluir an√°lisis geogr√°fico
            include_quality: Si incluir m√©tricas de calidad

        Returns:
            Reporte formateado como string
        """
        lines = [
            "üìä REPORTE INTEGRAL DE DATOS",
            "=" * 40,
            f"Fecha: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"Registros: {len(df):,}",
            f"Columnas: {len(df.columns)}",
            f"Memoria: {df.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB",
            ""
        ]

        # An√°lisis geogr√°fico
        if include_geographic:
            geo_cols = self.pattern_detector.detectar_columnas_geograficas(df)
            if geo_cols:
                lines.extend([
                    "üó∫Ô∏è  AN√ÅLISIS GEOGR√ÅFICO:",
                    f"Columnas geogr√°ficas detectadas: {len(geo_cols)}",
                ])

                for tipo, columna in geo_cols.items():
                    completitud = df[columna].notna().sum() / len(df) * 100
                    lines.append(f"  ‚Ä¢ {tipo}: {columna} ({completitud:.1f}% completo)")

                # Validaci√≥n UBIGEO si existe
                if 'ubigeo' in geo_cols:
                    validation = self.validate_geographic_data(df, geo_cols['ubigeo'])
                    lines.extend([
                        "",
                        f"Validaci√≥n UBIGEO: {'‚úÖ V√°lido' if validation.is_valid else '‚ùå Inv√°lido'}",
                        f"Cobertura: {validation.coverage_percentage:.1f}%"
                    ])

        # M√©tricas de calidad
        if include_quality:
            completeness = (1 - df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100

            lines.extend([
                "",
                "üìà M√âTRICAS DE CALIDAD:",
                f"Completitud general: {completeness:.1f}%",
            ])

            # Top columnas con valores faltantes
            missing_by_col = df.isnull().sum().sort_values(ascending=False)
            top_missing = missing_by_col[missing_by_col > 0].head(5)

            if not top_missing.empty:
                lines.append("Columnas con m√°s faltantes:")
                for col, missing in top_missing.items():
                    pct = missing / len(df) * 100
                    lines.append(f"  ‚Ä¢ {col}: {missing:,} ({pct:.1f}%)")

        lines.append("")
        return "\n".join(lines)